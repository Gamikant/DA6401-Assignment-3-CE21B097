{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ccfb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, SimpleRNN, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0da379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 44204\n",
      "Development examples: 4358\n",
      "Test examples: 4502\n",
      "\n",
      "Sample data (Devanagari -> Latin):\n",
      "अं -> an\n",
      "अंकगणित -> ankganit\n",
      "अंकल -> uncle\n",
      "अंकुर -> ankur\n",
      "अंकुरण -> ankuran\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load transliteration data from TSV files\n",
    "    Returns pairs of (native_script, latin_script)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                native_script = parts[0]  # Devanagari\n",
    "                latin_script = parts[1]   # Romanized\n",
    "                data.append((native_script, latin_script))\n",
    "    return data\n",
    "\n",
    "# Load Hindi transliteration data\n",
    "data_dir = \"dakshina_dataset_v1.0/hi/lexicons\"\n",
    "train_data = load_data(os.path.join(data_dir, \"hi.translit.sampled.train.tsv\"))\n",
    "dev_data = load_data(os.path.join(data_dir, \"hi.translit.sampled.dev.tsv\"))\n",
    "test_data = load_data(os.path.join(data_dir, \"hi.translit.sampled.test.tsv\"))\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Development examples: {len(dev_data)}\")\n",
    "print(f\"Test examples: {len(test_data)}\")\n",
    "\n",
    "# Display a few examples\n",
    "print(\"\\nSample data (Devanagari -> Latin):\")\n",
    "for i in range(5):\n",
    "    print(f\"{train_data[i][0]} -> {train_data[i][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a180a318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocabulary size: 28\n",
      "Output vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "def create_tokenizers(data):\n",
    "    \"\"\"\n",
    "    Create character-level tokenization for input and output sequences\n",
    "    \"\"\"\n",
    "    # Get all unique characters in input (Latin) and output (Devanagari) sequences\n",
    "    input_chars = set()\n",
    "    output_chars = set()\n",
    "    \n",
    "    for native, latin in data:\n",
    "        for char in latin:\n",
    "            input_chars.add(char)\n",
    "        for char in native:\n",
    "            output_chars.add(char)\n",
    "    \n",
    "    # Add special tokens\n",
    "    input_chars.add('\\t')  # Start token\n",
    "    input_chars.add('\\n')  # End token\n",
    "    output_chars.add('\\t')  # Start token\n",
    "    output_chars.add('\\n')  # End token\n",
    "    \n",
    "    # Create character-to-index mappings\n",
    "    input_char_to_idx = {char: i for i, char in enumerate(sorted(list(input_chars)))}\n",
    "    output_char_to_idx = {char: i for i, char in enumerate(sorted(list(output_chars)))}\n",
    "    \n",
    "    # Create index-to-character mappings\n",
    "    input_idx_to_char = {i: char for char, i in input_char_to_idx.items()}\n",
    "    output_idx_to_char = {i: char for char, i in output_char_to_idx.items()}\n",
    "    \n",
    "    return (input_char_to_idx, input_idx_to_char, \n",
    "            output_char_to_idx, output_idx_to_char)\n",
    "\n",
    "# Create tokenizers using all data\n",
    "all_data = train_data + dev_data + test_data\n",
    "input_char_to_idx, input_idx_to_char, output_char_to_idx, output_idx_to_char = create_tokenizers(all_data)\n",
    "\n",
    "print(f\"Input vocabulary size: {len(input_char_to_idx)}\")\n",
    "print(f\"Output vocabulary size: {len(output_char_to_idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90deb660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max input sequence length: 22\n",
      "Max output sequence length: 21\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(data, input_char_to_idx, output_char_to_idx, max_input_len=None, max_output_len=None):\n",
    "    \"\"\"\n",
    "    Convert character sequences to integer sequences\n",
    "    \"\"\"\n",
    "    # Determine max lengths if not provided\n",
    "    if max_input_len is None:\n",
    "        max_input_len = max([len(latin) for _, latin in data]) + 2  # +2 for start/end tokens\n",
    "    if max_output_len is None:\n",
    "        max_output_len = max([len(native) for native, _ in data]) + 2  # +2 for start/end tokens\n",
    "    \n",
    "    encoder_input_data = []\n",
    "    decoder_input_data = []\n",
    "    decoder_target_data = []\n",
    "    \n",
    "    for native, latin in data:\n",
    "        # Encoder input (Latin script with start/end tokens)\n",
    "        input_text = '\\t' + latin + '\\n'\n",
    "        encoder_input = [input_char_to_idx[char] for char in input_text]\n",
    "        # Pad encoder input\n",
    "        encoder_input = encoder_input + [0] * (max_input_len - len(encoder_input))\n",
    "        encoder_input_data.append(encoder_input)\n",
    "        \n",
    "        # Decoder input (Devanagari script with start token)\n",
    "        target_text = '\\t' + native + '\\n'\n",
    "        decoder_input = [output_char_to_idx[char] for char in target_text[:-1]]  # exclude the last character\n",
    "        # Pad decoder input\n",
    "        decoder_input = decoder_input + [0] * (max_output_len - 1 - len(decoder_input))\n",
    "        decoder_input_data.append(decoder_input)\n",
    "        \n",
    "        # Decoder target (Devanagari script shifted by one timestep)\n",
    "        decoder_target = [output_char_to_idx[char] for char in target_text[1:]]  # exclude the first character\n",
    "        # Pad decoder target\n",
    "        decoder_target = decoder_target + [0] * (max_output_len - 1 - len(decoder_target))\n",
    "        decoder_target_data.append(decoder_target)\n",
    "    \n",
    "    return (np.array(encoder_input_data), \n",
    "            np.array(decoder_input_data), \n",
    "            np.array(decoder_target_data), \n",
    "            max_input_len, max_output_len)\n",
    "\n",
    "# Find max sequence lengths\n",
    "max_input_len = max([len(latin) for _, latin in all_data]) + 2\n",
    "max_output_len = max([len(native) for native, _ in all_data]) + 2\n",
    "\n",
    "print(f\"Max input sequence length: {max_input_len}\")\n",
    "print(f\"Max output sequence length: {max_output_len}\")\n",
    "\n",
    "# Preprocess data\n",
    "encoder_input_train, decoder_input_train, decoder_target_train, _, _ = preprocess_data(\n",
    "    train_data, input_char_to_idx, output_char_to_idx, max_input_len, max_output_len)\n",
    "encoder_input_dev, decoder_input_dev, decoder_target_dev, _, _ = preprocess_data(\n",
    "    dev_data, input_char_to_idx, output_char_to_idx, max_input_len, max_output_len)\n",
    "encoder_input_test, decoder_input_test, decoder_target_test, _, _ = preprocess_data(\n",
    "    test_data, input_char_to_idx, output_char_to_idx, max_input_len, max_output_len)\n",
    "\n",
    "# Convert target data to one-hot encoding\n",
    "decoder_target_train_onehot = tf.keras.utils.to_categorical(\n",
    "    decoder_target_train, num_classes=len(output_char_to_idx))\n",
    "decoder_target_dev_onehot = tf.keras.utils.to_categorical(\n",
    "    decoder_target_dev, num_classes=len(output_char_to_idx))\n",
    "decoder_target_test_onehot = tf.keras.utils.to_categorical(\n",
    "    decoder_target_test, num_classes=len(output_char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd449c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,168</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm_0      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ encoder_embeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm_0      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ decoder_embeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,705</span> │ decoder_lstm_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m7,168\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m16,640\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm_0      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m525,312\u001b[0m │ encoder_embeddin… │\n",
       "│ (\u001b[38;5;33mLSTM\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm_0      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m525,312\u001b[0m │ decoder_embeddin… │\n",
       "│ (\u001b[38;5;33mLSTM\u001b[0m)              │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm_0[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm_0[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)  │     \u001b[38;5;34m16,705\u001b[0m │ decoder_lstm_0[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,091,137</span> (4.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,091,137\u001b[0m (4.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,091,137</span> (4.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,091,137\u001b[0m (4.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_seq2seq_model(input_vocab_size, output_vocab_size, \n",
    "                         embedding_dim=256, hidden_dim=256, \n",
    "                         cell_type='lstm', num_encoder_layers=1, num_decoder_layers=1):\n",
    "    \"\"\"\n",
    "    Create a sequence-to-sequence model with the specified parameters\n",
    "    \n",
    "    Args:\n",
    "        input_vocab_size: Size of input vocabulary\n",
    "        output_vocab_size: Size of output vocabulary\n",
    "        embedding_dim: Dimension of character embeddings\n",
    "        hidden_dim: Dimension of hidden states in RNN\n",
    "        cell_type: Type of RNN cell ('rnn', 'lstm', or 'gru')\n",
    "        num_encoder_layers: Number of layers in encoder\n",
    "        num_decoder_layers: Number of layers in decoder\n",
    "        \n",
    "    Returns:\n",
    "        model: Keras model\n",
    "    \"\"\"\n",
    "    # Define RNN cell based on type\n",
    "    if cell_type.lower() == 'lstm':\n",
    "        RNN = LSTM\n",
    "    elif cell_type.lower() == 'gru':\n",
    "        RNN = GRU\n",
    "    elif cell_type.lower() == 'rnn':\n",
    "        RNN = SimpleRNN\n",
    "    else:\n",
    "        raise ValueError(\"cell_type must be one of 'rnn', 'lstm', or 'gru'\")\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
    "    encoder_embedding = Embedding(input_vocab_size, embedding_dim, name='encoder_embedding')(encoder_inputs)\n",
    "    \n",
    "    encoder_outputs = encoder_embedding\n",
    "    encoder_states = []\n",
    "    \n",
    "    # Add encoder layers\n",
    "    for i in range(num_encoder_layers):\n",
    "        return_sequences = i < num_encoder_layers - 1\n",
    "        return_state = i == num_encoder_layers - 1\n",
    "        \n",
    "        if cell_type.lower() == 'lstm':\n",
    "            encoder = RNN(hidden_dim, return_sequences=return_sequences, return_state=return_state, \n",
    "                          name=f'encoder_lstm_{i}')\n",
    "            if return_state:\n",
    "                encoder_outputs, state_h, state_c = encoder(encoder_outputs)\n",
    "                encoder_states = [state_h, state_c]\n",
    "            else:\n",
    "                encoder_outputs = encoder(encoder_outputs)\n",
    "        else:\n",
    "            encoder = RNN(hidden_dim, return_sequences=return_sequences, return_state=return_state, \n",
    "                          name=f'encoder_{cell_type}_{i}')\n",
    "            if return_state:\n",
    "                encoder_outputs, state_h = encoder(encoder_outputs)\n",
    "                encoder_states = [state_h]\n",
    "            else:\n",
    "                encoder_outputs = encoder(encoder_outputs)\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
    "    decoder_embedding = Embedding(output_vocab_size, embedding_dim, name='decoder_embedding')(decoder_inputs)\n",
    "    \n",
    "    decoder_outputs = decoder_embedding\n",
    "    decoder_states = []\n",
    "    \n",
    "    # Add decoder layers\n",
    "    for i in range(num_decoder_layers):\n",
    "        return_sequences = True\n",
    "        return_state = True\n",
    "        \n",
    "        if cell_type.lower() == 'lstm':\n",
    "            decoder = RNN(hidden_dim, return_sequences=return_sequences, return_state=return_state, \n",
    "                          name=f'decoder_lstm_{i}')\n",
    "            if i == 0:\n",
    "                decoder_outputs, _, _ = decoder(decoder_outputs, initial_state=encoder_states)\n",
    "            else:\n",
    "                decoder_outputs, _, _ = decoder(decoder_outputs)\n",
    "        else:\n",
    "            decoder = RNN(hidden_dim, return_sequences=return_sequences, return_state=return_state, \n",
    "                          name=f'decoder_{cell_type}_{i}')\n",
    "            if i == 0:\n",
    "                decoder_outputs, _ = decoder(decoder_outputs, initial_state=encoder_states)\n",
    "            else:\n",
    "                decoder_outputs, _ = decoder(decoder_outputs)\n",
    "    \n",
    "    # Output layer\n",
    "    decoder_dense = Dense(output_vocab_size, activation='softmax', name='decoder_dense')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a model with default parameters\n",
    "model = create_seq2seq_model(\n",
    "    input_vocab_size=len(input_char_to_idx),\n",
    "    output_vocab_size=len(output_char_to_idx),\n",
    "    embedding_dim=256,\n",
    "    hidden_dim=256,\n",
    "    cell_type='lstm',\n",
    "    num_encoder_layers=1,\n",
    "    num_decoder_layers=1\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f61ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_models(model, cell_type='lstm'):\n",
    "    \"\"\"\n",
    "    Create encoder and decoder models for inference\n",
    "    \"\"\"\n",
    "    if cell_type.lower() == 'lstm':\n",
    "        # Encoder model\n",
    "        encoder_inputs = Input(shape=(None,), name='encoder_inference_inputs')\n",
    "        encoder = model.get_layer('encoder_lstm_0')\n",
    "        encoder_embedding = model.get_layer('encoder_embedding')\n",
    "        \n",
    "        x = encoder_embedding(encoder_inputs)\n",
    "        encoder_outputs, state_h, state_c = encoder(x)\n",
    "        encoder_states = [state_h, state_c]\n",
    "        \n",
    "        encoder_model = Model(encoder_inputs, encoder_states)\n",
    "        \n",
    "        # Decoder model\n",
    "        decoder_inputs = Input(shape=(None,), name='decoder_inference_inputs')\n",
    "        decoder_state_input_h = Input(shape=(encoder.units,))\n",
    "        decoder_state_input_c = Input(shape=(encoder.units,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        \n",
    "        decoder_embedding = model.get_layer('decoder_embedding')\n",
    "        decoder_lstm = model.get_layer('decoder_lstm_0')\n",
    "        decoder_dense = model.get_layer('decoder_dense')\n",
    "        \n",
    "        decoder_outputs = decoder_embedding(decoder_inputs)\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "            decoder_outputs, initial_state=decoder_states_inputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "        decoder_model = Model(\n",
    "            [decoder_inputs] + decoder_states_inputs,\n",
    "            [decoder_outputs] + decoder_states\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        # For RNN/GRU\n",
    "        encoder_inputs = Input(shape=(None,), name='encoder_inference_inputs')\n",
    "        encoder = model.get_layer(f'encoder_{cell_type}_0')\n",
    "        encoder_embedding = model.get_layer('encoder_embedding')\n",
    "        \n",
    "        x = encoder_embedding(encoder_inputs)\n",
    "        encoder_outputs, state_h = encoder(x)\n",
    "        encoder_states = [state_h]\n",
    "        \n",
    "        encoder_model = Model(encoder_inputs, encoder_states)\n",
    "        \n",
    "        # Decoder model\n",
    "        decoder_inputs = Input(shape=(None,), name='decoder_inference_inputs')\n",
    "        decoder_state_input_h = Input(shape=(encoder.units,))\n",
    "        decoder_states_inputs = [decoder_state_input_h]\n",
    "        \n",
    "        decoder_embedding = model.get_layer('decoder_embedding')\n",
    "        decoder_rnn = model.get_layer(f'decoder_{cell_type}_0')\n",
    "        decoder_dense = model.get_layer('decoder_dense')\n",
    "        \n",
    "        decoder_outputs = decoder_embedding(decoder_inputs)\n",
    "        decoder_outputs, state_h = decoder_rnn(\n",
    "            decoder_outputs, initial_state=decoder_states_inputs)\n",
    "        decoder_states = [state_h]\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "        decoder_model = Model(\n",
    "            [decoder_inputs] + decoder_states_inputs,\n",
    "            [decoder_outputs] + decoder_states\n",
    "        )\n",
    "    \n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "def train_model(model, encoder_input_train, decoder_input_train, decoder_target_train_onehot,\n",
    "               encoder_input_dev, decoder_input_dev, decoder_target_dev_onehot,\n",
    "               input_vocab_size, output_vocab_size, embedding_dim, hidden_dim, cell_type,\n",
    "               epochs=20, batch_size=64):\n",
    "    \"\"\"\n",
    "    Train the sequence-to-sequence model\n",
    "    \"\"\"\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"seq2seq-transliteration\",\n",
    "        name=f\"seq2seq-{cell_type}\",\n",
    "        config={\n",
    "            \"cell_type\": cell_type,\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"hidden_dim\": hidden_dim,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Create callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f\"checkpoints/seq2seq_{cell_type}_model.keras\",\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [encoder_input_train, decoder_input_train],\n",
    "        decoder_target_train_onehot,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=([encoder_input_dev, decoder_input_dev], decoder_target_dev_onehot),\n",
    "        callbacks=[checkpoint, early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Create inference models\n",
    "    encoder_model, decoder_model = create_inference_models(model, cell_type)\n",
    "    \n",
    "    return decoder_model, history\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq, encoder_model, decoder_model, \n",
    "                   output_char_to_idx, output_idx_to_char,\n",
    "                   max_output_len, cell_type):\n",
    "    \"\"\"\n",
    "    Decode an input sequence using the trained encoder and decoder models\n",
    "    \"\"\"\n",
    "    # Encode the input as state vectors\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "    \n",
    "    # Generate empty target sequence of length 1, with the start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = output_char_to_idx['\\t']\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    stop_condition = False\n",
    "    decoded_seq = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        # Predict next character\n",
    "        if cell_type.lower() == 'lstm':\n",
    "            output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
    "            states_value = [h, c]\n",
    "        else:\n",
    "            output_tokens, h = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
    "            states_value = [h]\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_char = output_idx_to_char[sampled_token_index]\n",
    "        \n",
    "        # Exit condition: either hit max length or find stop character\n",
    "        if sampled_char == '\\n' or len(decoded_seq) >= max_output_len - 2:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_seq += sampled_char\n",
    "        \n",
    "        # Update the target sequence (length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "    \n",
    "    return decoded_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51576ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(encoder_model, decoder_model, data, encoder_input_data,\n",
    "                  output_char_to_idx, output_idx_to_char,\n",
    "                  max_output_len, cell_type):\n",
    "    \"\"\"\n",
    "    Evaluate the model on given data\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = len(data)\n",
    "    predictions = []\n",
    "    \n",
    "    for i, (native, latin) in enumerate(data):\n",
    "        input_seq = encoder_input_data[i:i+1]\n",
    "        predicted = decode_sequence(\n",
    "            input_seq, encoder_model, decoder_model,\n",
    "            output_char_to_idx, output_idx_to_char,\n",
    "            max_output_len, cell_type\n",
    "        )\n",
    "        predictions.append((native, latin, predicted))\n",
    "        \n",
    "        if predicted == native:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28767f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, encoder_input_train, decoder_input_train, decoder_target_train_onehot,\n",
    "               encoder_input_dev, decoder_input_dev, decoder_target_dev_onehot,\n",
    "               input_vocab_size, output_vocab_size, embedding_dim, hidden_dim, cell_type,\n",
    "               epochs=20, batch_size=64):\n",
    "    \"\"\"\n",
    "    Train the sequence-to-sequence model\n",
    "    \"\"\"\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"seq2seq-transliteration\",\n",
    "        name=f\"seq2seq-{cell_type}\",\n",
    "        config={\n",
    "            \"cell_type\": cell_type,\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"hidden_dim\": hidden_dim,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Create a valid checkpoint file path\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"seq2seq_{cell_type}_model.keras\")\n",
    "    \n",
    "    # Define callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [encoder_input_train, decoder_input_train],\n",
    "        decoder_target_train_onehot,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=([encoder_input_dev, decoder_input_dev], decoder_target_dev_onehot),\n",
    "        callbacks=[checkpoint, early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Create inference models\n",
    "    encoder_model, decoder_model = create_inference_models(model, cell_type)\n",
    "    \n",
    "    return model, encoder_model, decoder_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23032426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1(a) Total computations: 5765120\n",
      "1(b) Total parameters: 339556\n"
     ]
    }
   ],
   "source": [
    "def compute_total_computations(m, k, T, V):\n",
    "    \"\"\"\n",
    "    Calculate the total number of computations in the seq2seq model\n",
    "    \n",
    "    Args:\n",
    "        m: Input embedding size\n",
    "        k: Hidden state size\n",
    "        T: Sequence length\n",
    "        V: Vocabulary size\n",
    "    \"\"\"\n",
    "    # Encoder computations\n",
    "    # Embedding layer: T * m (for each character in the input sequence)\n",
    "    # RNN layer: T * (m*k + k*k) (for each character, compute from input and previous state)\n",
    "    encoder_computations = T * m + T * (m*k + k*k)\n",
    "    \n",
    "    # Decoder computations\n",
    "    # Embedding layer: T * m (for each character in the output sequence)\n",
    "    # RNN layer: T * (m*k + k*k) (for each character, compute from input and previous state)\n",
    "    # Output layer: T * k*V (for each character, compute probability for each character in vocab)\n",
    "    decoder_computations = T * m + T * (m*k + k*k) + T * k * V\n",
    "    \n",
    "    total_computations = encoder_computations + decoder_computations\n",
    "    return total_computations\n",
    "\n",
    "def compute_total_parameters(m, k, T, V):\n",
    "    \"\"\"\n",
    "    Calculate the total number of parameters in the seq2seq model\n",
    "    \n",
    "    Args:\n",
    "        m: Input embedding size\n",
    "        k: Hidden state size\n",
    "        T: Sequence length\n",
    "        V: Vocabulary size\n",
    "    \"\"\"\n",
    "    # Encoder parameters\n",
    "    # Embedding layer: V * m (one embedding vector for each character in vocab)\n",
    "    # RNN layer: (m*k + k*k + k) (weights for input, weights for hidden state, bias)\n",
    "    encoder_parameters = V * m + (m*k + k*k + k)\n",
    "    \n",
    "    # Decoder parameters\n",
    "    # Embedding layer: V * m (one embedding vector for each character in vocab)\n",
    "    # RNN layer: (m*k + k*k + k) (weights for input, weights for hidden state, bias)\n",
    "    # Output layer: k*V + V (weights and bias)\n",
    "    decoder_parameters = V * m + (m*k + k*k + k) + (k*V + V)\n",
    "    \n",
    "    total_parameters = encoder_parameters + decoder_parameters\n",
    "    return total_parameters\n",
    "\n",
    "# Answer to question 1(a)\n",
    "m = 256  # embedding size\n",
    "k = 256  # hidden state size\n",
    "T = 20   # approximate sequence length\n",
    "V = 100  # approximate vocabulary size\n",
    "\n",
    "total_computations = compute_total_computations(m, k, T, V)\n",
    "print(f\"1(a) Total computations: {total_computations}\")\n",
    "\n",
    "# Answer to question 1(b)\n",
    "total_parameters = compute_total_parameters(m, k, T, V)\n",
    "print(f\"1(b) Total parameters: {total_parameters}\")\n",
    "\n",
    "def character_level_accuracy(true_text, pred_text):\n",
    "    \"\"\"\n",
    "    Calculate character-level accuracy between two strings\n",
    "    \"\"\"\n",
    "    min_len = min(len(true_text), len(pred_text))\n",
    "    matches = sum(true_text[i] == pred_text[i] for i in range(min_len))\n",
    "    return matches / max(len(true_text), len(pred_text))\n",
    "\n",
    "def evaluate_model_with_char_accuracy(encoder_model, decoder_model, data, encoder_input_data,\n",
    "                                     output_char_to_idx, output_idx_to_char,\n",
    "                                     max_output_len, cell_type):\n",
    "    \"\"\"\n",
    "    Evaluate the model on given data with both word and character accuracy\n",
    "    \"\"\"\n",
    "    word_correct = 0\n",
    "    total_char_accuracy = 0\n",
    "    total = len(data)\n",
    "    predictions = []\n",
    "    \n",
    "    for i, (native, latin) in enumerate(data):\n",
    "        input_seq = encoder_input_data[i:i+1]\n",
    "        predicted = decode_sequence(\n",
    "            input_seq, encoder_model, decoder_model,\n",
    "            output_char_to_idx, output_idx_to_char,\n",
    "            max_output_len, cell_type\n",
    "        )\n",
    "        predictions.append((native, latin, predicted))\n",
    "        \n",
    "        # Word-level accuracy\n",
    "        if predicted == native:\n",
    "            word_correct += 1\n",
    "            \n",
    "        # Character-level accuracy\n",
    "        char_acc = character_level_accuracy(native, predicted)\n",
    "        total_char_accuracy += char_acc\n",
    "    \n",
    "    word_accuracy = word_correct / total\n",
    "    avg_char_accuracy = total_char_accuracy / total\n",
    "    \n",
    "    return word_accuracy, avg_char_accuracy, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f7bd674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.6744 - loss: 1.4344\n",
      "Epoch 1: val_loss improved from inf to 0.92819, saving model to checkpoints/seq2seq_lstm_model.keras\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 122ms/step - accuracy: 0.6746 - loss: 1.4327 - val_accuracy: 0.7491 - val_loss: 0.9282\n",
      "Epoch 2/2\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.7487 - loss: 0.9225\n",
      "Epoch 2: val_loss improved from 0.92819 to 0.77273, saving model to checkpoints/seq2seq_lstm_model.keras\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 130ms/step - accuracy: 0.7488 - loss: 0.9222 - val_accuracy: 0.7831 - val_loss: 0.7727\n",
      "Restoring model weights from the end of the best epoch: 2.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 22\u001b[0m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m create_seq2seq_model(\n\u001b[0;32m      9\u001b[0m     input_vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(input_char_to_idx),\n\u001b[0;32m     10\u001b[0m     output_vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(output_char_to_idx),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     cell_type\u001b[38;5;241m=\u001b[39mcell_type\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     17\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     18\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     20\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m model, encoder_model, decoder_model, history \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[0;32m     23\u001b[0m     model, encoder_input_train, decoder_input_train, decoder_target_train_onehot,\n\u001b[0;32m     24\u001b[0m     encoder_input_dev, decoder_input_dev, decoder_target_dev_onehot,\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mlen\u001b[39m(input_char_to_idx), \u001b[38;5;28mlen\u001b[39m(output_char_to_idx), embedding_dim, hidden_dim, cell_type,\n\u001b[0;32m     26\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     30\u001b[0m word_accuracy, char_accuracy, dev_predictions \u001b[38;5;241m=\u001b[39m evaluate_model_with_char_accuracy(\n\u001b[0;32m     31\u001b[0m     encoder_model, decoder_model, dev_data, encoder_input_dev,\n\u001b[0;32m     32\u001b[0m     output_char_to_idx, output_idx_to_char,\n\u001b[0;32m     33\u001b[0m     max_output_len, cell_type\n\u001b[0;32m     34\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "# Train a simple model\n",
    "embedding_dim = 256\n",
    "hidden_dim = 256\n",
    "cell_type = 'lstm'\n",
    "epochs = 2\n",
    "batch_size = 256\n",
    "\n",
    "model = create_seq2seq_model(\n",
    "    input_vocab_size=len(input_char_to_idx),\n",
    "    output_vocab_size=len(output_char_to_idx),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    cell_type=cell_type\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model, encoder_model, decoder_model, history = train_model(\n",
    "    model, encoder_input_train, decoder_input_train, decoder_target_train_onehot,\n",
    "    encoder_input_dev, decoder_input_dev, decoder_target_dev_onehot,\n",
    "    len(input_char_to_idx), len(output_char_to_idx), embedding_dim, hidden_dim, cell_type,\n",
    "    epochs=epochs, batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "word_accuracy, char_accuracy, dev_predictions = evaluate_model_with_char_accuracy(\n",
    "    encoder_model, decoder_model, dev_data, encoder_input_dev,\n",
    "    output_char_to_idx, output_idx_to_char,\n",
    "    max_output_len, cell_type\n",
    ")\n",
    "\n",
    "print(f\"Development word accuracy: {word_accuracy:.4f}\")\n",
    "print(f\"Development character accuracy: {char_accuracy:.4f}\")\n",
    "\n",
    "# Show some predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "for i in range(10):\n",
    "    native, latin, predicted = dev_predictions[i]\n",
    "    print(f\"Input: {latin}, Target: {native}, Predicted: {predicted}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff2bf0f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
