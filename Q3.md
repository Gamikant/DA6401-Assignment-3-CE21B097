<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" class="logo" width="120"/>

# Write it like I'm writing it. I just looked at the plots and answering question 3 after pasting these plots

Okay, I've pasted the plots from my Weights \& Biases sweep below. Based on these visualizations, here are some of my observations regarding the model's performance and hyperparameter sensitivity for this Hindi transliteration task:

*(Imagine the user has pasted Image 1, Image 2, Image 3, and Image 4 here in their report)*

**Observations from the Hyperparameter Sweep:**

* **Cell Type is King - GRU Shows Strong Performance:**
    * The most impactful hyperparameter appears to be the `cell_type`. Looking at the "Parameter importance" plot (Image 1), `cell_type.value_GRU` has a significant positive correlation with my `val_exact_match_accuracy`. In contrast, `cell_type.value_LSTM` shows a strong negative correlation, and `cell_type.value_RNN` (vanilla RNN) also negatively correlates with the accuracy.
    * This is further supported by the "Parallel Coordinates" plot (Image 2). The lines representing my highest accuracy runs (the warmer colored ones, achieving `val_exact_match_accuracy` above ~0.25) are almost exclusively associated with the GRU cell type.
    * The epoch-wise `val_accuracy` plot (Image 4, top middle graph) also tells a story. Runs like "distinctive-sweep-9" and "scarlet-sweep-10," which reached the highest validation accuracies (around 0.28), likely correspond to my GRU configurations. These converged well. Conversely, runs like "stilted-sweep-2" and "decent-sweep-1" performed much worse, probably representing the vanilla RNN or less effective LSTM setups.
    * This strongly suggests that GRU cells were the most effective for this task, outperforming LSTMs and basic RNNs within the configurations I tested.
* **Larger Model Capacity (Hidden \& Embedding Size) is Generally Better:**
    * The "Parameter importance" plot (Image 1) indicates positive correlations for both `hidden_size` and `input_embedding_size` with `val_exact_match_accuracy`.
    * This observation is backed by the "Parallel Coordinates" plot (Image 2), where many of the top-performing runs (warm-colored lines) utilized larger values for `hidden_size` (e.g., 256) and `input_embedding_size` (e.g., 128 or 256).
    * So, the hypothesis that "using smaller sizes for the hidden layer does not give good results" seems to hold true based on my sweep.
* **Higher Learning Rates (within the tested range) Preferred:**
    * `learning_rate` was very influential, showing the strongest positive correlation with `val_exact_match_accuracy` in the "Parameter importance" plot (Image 1).
    * The "Parallel Coordinates" plot (Image 2) shows that my best-performing runs often used `learning_rate` values towards the higher end of the range I swept over (e.g., values around 0.001).
* **Impact of Dropout is Ambiguous in this Sweep:**
    * The "Parameter importance" plot (Image 1) doesn't rank `dropout_rate` as a highly impactful parameter for accuracy in this sweep.
    * In the "Parallel Coordinates" plot (Image 2), high-accuracy runs are spread across various `dropout_rate` values.
    * Therefore, the statement "dropout leads to better performance" isn't strongly confirmed or refuted by these plots alone. While dropout is generally a good regularizer, its optimal value and impact here seem to be more nuanced or possibly interact with other settings.
* **Number of Encoder/Decoder Layers:**
    * `decoder_layers` shows a slight positive correlation with accuracy in the "Parameter importance" plot (Image 1).
    * However, the "Parallel Coordinates" plot (Image 2) doesn't show a very strong preference; successful runs are seen with both 1 and 2 layers for both encoder and decoder. Adding more layers (within my tested range of 1-2) didn't consistently push runs to the top.
* **Optimizer Choice - Adam Seems Effective:**
    * The "Parameter importance" plot (Image 1) suggests a positive correlation for `optimizer.value_adam`. The "Parallel Coordinates" plot (Image 2) also has several high-accuracy runs using Adam.
* **Convergence Time and Cell Types:**
    * Regarding the hypothesis "RNN based model takes longer time to converge than GRU or LSTM": The epoch-wise `val_accuracy` plots (Image 4) show that the high-performing runs (likely GRUs, such as "distinctive-sweep-9") not only achieved better accuracy but also appeared to converge to these higher accuracies effectively, generally within 20-30 epochs. Lower-performing runs (possibly RNNs or LSTMs that didn't do well) showed slower progress to their much lower peak accuracies or stalled early. This indirectly supports that more effective cell types like GRU lead to better and efficient convergence to a superior result.
* **A Note on `beam_size` and the Metric Used:**
    * I should clarify that for this sweep, I used the Keras `val_accuracy` (obtained during `model.fit`) as the `val_exact_match_accuracy` metric for convenience, and I had commented out the actual beam search evaluation part. Therefore, the `beam_size` hyperparameter shown in these plots did not actually influence the reported accuracy metric. Any correlation observed for `beam_size` is coincidental in the context of the metric I tracked.
* **Overall Sweep Progression:**
    * The "val_exact_match_accuracy v. created" plot (Image 3) is encouraging, as it shows that the sweep did find configurations that improved the validation accuracy over the course of the experiments.

These are my main takeaways from analyzing the sweep results. The GRU cell type, along with larger embedding/hidden sizes and a relatively higher learning rate, seems to be a promising direction.

<div style="text-align: center">‚ÅÇ</div>

[^1]: hi.translit.sampled.dev.txt

[^2]: hi.translit.sampled.test.txt

[^3]: hi.translit.sampled.train.txt

[^4]: Assignment-3.ipynb

[^5]: image.jpg

[^6]: image.jpg

[^7]: Chart-2.jpg

[^8]: Chart-1.jpg

