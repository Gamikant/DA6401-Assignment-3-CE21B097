{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefee210-5bea-417a-af3c-02b9a371c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Libraries and Import Modules\n",
    "# !pip install wandb tensorflow numpy pandas scikit-learn matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, GRU, Dense, Dropout, Concatenate, AdditiveAttention, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split # Though not directly used in sweep, good to have\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker # For heatmap ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bce6ff2-1bd0-4640-a67a-d94d2d6967ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: ce21b097 (ce21b097-indian-institute-of-technology-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2: Wandb Login\n",
    "# Run this cell and follow the instructions to log in to your W&B account.\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79785683-5912-4dfd-83b0-2736dd1b799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 44202\n",
      "Validation samples: 4358\n",
      "Test samples: 4502\n",
      "\n",
      "Sample training data:\n",
      "Input: an, Target: अं\n",
      "Input: ankganit, Target: अंकगणित\n",
      "Input: uncle, Target: अंकल\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Loading and Initial Parsing\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Loads data from a TSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep='\\t', header=None, on_bad_lines='skip', names=['native', 'roman', 'count'])\n",
    "        df.dropna(subset=['native', 'roman'], inplace=True)\n",
    "        input_texts = df['roman'].astype(str).tolist()\n",
    "        target_texts = df['native'].astype(str).tolist()\n",
    "        return input_texts, target_texts\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from {filepath}: {e}\")\n",
    "        return [], []\n",
    "\n",
    "# --- Define file paths ---\n",
    "dataset_base_dir = 'dakshina_dataset_v1.0' \n",
    "language = 'hi' # Hindi\n",
    "\n",
    "train_file = os.path.join(dataset_base_dir, language, 'lexicons', f'{language}.translit.sampled.train.tsv')\n",
    "dev_file = os.path.join(dataset_base_dir, language, 'lexicons', f'{language}.translit.sampled.dev.tsv')\n",
    "test_file = os.path.join(dataset_base_dir, language, 'lexicons', f'{language}.translit.sampled.test.tsv')\n",
    "\n",
    "# Load data\n",
    "input_texts_train_full, target_texts_train_full = load_data(train_file)\n",
    "input_texts_val, target_texts_val = load_data(dev_file)\n",
    "input_texts_test, target_texts_test = load_data(test_file)\n",
    "\n",
    "# For sweeps, we can use the same split as before or the full training data\n",
    "input_texts_train, target_texts_train = input_texts_train_full, target_texts_train_full\n",
    "\n",
    "print(f\"Training samples: {len(input_texts_train)}\")\n",
    "print(f\"Validation samples: {len(input_texts_val)}\")\n",
    "print(f\"Test samples: {len(input_texts_test)}\")\n",
    "\n",
    "if len(input_texts_train) > 0 and len(target_texts_train) > 0:\n",
    "    print(\"\\nSample training data:\")\n",
    "    for i in range(min(3, len(input_texts_train))):\n",
    "        print(f\"Input: {input_texts_train[i]}, Target: {target_texts_train[i]}\")\n",
    "else:\n",
    "    print(\"No training data loaded. Please check file paths and content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e457aa-18cb-4d81-b272-decd90740d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique input tokens: 26\n",
      "Number of unique output tokens: 65\n",
      "Max sequence length for inputs: 20\n",
      "Max sequence length for outputs: 21\n",
      "\n",
      "Shape of encoder_input_train: (44202, 20)\n",
      "Shape of decoder_input_train: (44202, 21)\n",
      "Shape of decoder_target_train: (44202, 21, 65)\n",
      "Shape of encoder_input_val: (4358, 20)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Data Preprocessing - Vocabulary, Tokenization, Padding\n",
    "\n",
    "# --- Character sets and tokenization ---\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "# Use all available text data (train_full, val, test) to build a comprehensive vocabulary\n",
    "# This ensures that characters encountered in val/test are known.\n",
    "all_input_texts_for_vocab = input_texts_train_full + input_texts_val + input_texts_test\n",
    "all_target_texts_for_vocab = target_texts_train_full + target_texts_val + target_texts_test\n",
    "\n",
    "\n",
    "for text in all_input_texts_for_vocab:\n",
    "    for char in str(text): # Ensure text is string\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "\n",
    "for text in all_target_texts_for_vocab:\n",
    "    for char in str(text): # Ensure text is string\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "# Add special tokens\n",
    "SOS_TOKEN = '\\t' # Start Of Sequence\n",
    "EOS_TOKEN = '\\n' # End Of Sequence\n",
    "target_characters.add(SOS_TOKEN)\n",
    "target_characters.add(EOS_TOKEN)\n",
    "\n",
    "\n",
    "input_char_list = sorted(list(input_characters))\n",
    "target_char_list = sorted(list(target_characters))\n",
    "\n",
    "num_encoder_tokens = len(input_char_list)\n",
    "num_decoder_tokens = len(target_char_list)\n",
    "\n",
    "input_token_index = {char: i for i, char in enumerate(input_char_list)}\n",
    "target_token_index = {char: i for i, char in enumerate(target_char_list)}\n",
    "\n",
    "reverse_input_char_index = {i: char for char, i in input_token_index.items()}\n",
    "reverse_target_char_index = {i: char for char, i in target_token_index.items()}\n",
    "\n",
    "# Determine max sequence lengths from all data splits\n",
    "all_input_texts_combined = input_texts_train_full + input_texts_val + input_texts_test\n",
    "all_target_texts_combined = target_texts_train_full + target_texts_val + target_texts_test\n",
    "\n",
    "max_encoder_seq_length = max(len(str(text)) for text in all_input_texts_combined)\n",
    "max_decoder_seq_length = max(len(str(text)) for text in all_target_texts_combined) + 2 # +2 for SOS and EOS\n",
    "\n",
    "print(f\"\\nNumber of unique input tokens: {num_encoder_tokens}\")\n",
    "print(f\"Number of unique output tokens: {num_decoder_tokens}\")\n",
    "print(f\"Max sequence length for inputs: {max_encoder_seq_length}\")\n",
    "print(f\"Max sequence length for outputs: {max_decoder_seq_length}\")\n",
    "\n",
    "\n",
    "# --- Vectorize the data ---\n",
    "def vectorize_data(input_texts, target_texts):\n",
    "    encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype=\"float32\")\n",
    "    decoder_input_data = np.zeros((len(target_texts), max_decoder_seq_length), dtype=\"float32\")\n",
    "    decoder_target_data = np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(str(input_text)): # Ensure input_text is string\n",
    "            if t < max_encoder_seq_length and char in input_token_index:\n",
    "                 encoder_input_data[i, t] = input_token_index[char]\n",
    "        \n",
    "        processed_target_text = SOS_TOKEN + str(target_text) + EOS_TOKEN # Ensure target_text is string\n",
    "        for t, char in enumerate(processed_target_text):\n",
    "            if t < max_decoder_seq_length:\n",
    "                if char in target_token_index:\n",
    "                    decoder_input_data[i, t] = target_token_index[char]\n",
    "                    if t > 0: \n",
    "                        decoder_target_data[i, t - 1, target_token_index[char]] = 1.0 \n",
    "    \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "\n",
    "# Vectorize the training and validation sets (using the 'train' subset for initial sweep training if preferred)\n",
    "encoder_input_train, decoder_input_train, decoder_target_train = vectorize_data(input_texts_train, target_texts_train)\n",
    "encoder_input_val, decoder_input_val, decoder_target_val = vectorize_data(input_texts_val, target_texts_val)\n",
    "# Test data will be vectorized later before final evaluation.\n",
    "\n",
    "print(\"\\nShape of encoder_input_train:\", encoder_input_train.shape)\n",
    "print(\"Shape of decoder_input_train:\", decoder_input_train.shape)\n",
    "print(\"Shape of decoder_target_train:\", decoder_target_train.shape)\n",
    "print(\"Shape of encoder_input_val:\", encoder_input_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a6ac9c3-d4af-44b5-b215-3aa1e0623755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Bahdanau Attention Layer Definition\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units) # For encoder outputs\n",
    "        self.W2 = tf.keras.layers.Dense(units) # For decoder hidden state\n",
    "        self.V = tf.keras.layers.Dense(1)      # To compute the score\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query shape == (batch_size, hidden_size) (decoder hidden state)\n",
    "        # values shape == (batch_size, max_len, hidden_size) (encoder outputs)\n",
    "\n",
    "        # Expand query to broadcast addition along sequence length dimension\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_len, 1)\n",
    "        # self.V applied to sum of W1(values) and W2(query)\n",
    "        # W1(values) shape == (batch_size, max_len, units)\n",
    "        # W2(query_with_time_axis) shape == (batch_size, 1, units)\n",
    "        # tf.nn.tanh(...) shape == (batch_size, max_len, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(query_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_len, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322d97e5-1400-486a-9c64-d87704317b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Attention Model Building Function\n",
    "\n",
    "def build_attention_seq2seq_model(config):\n",
    "    # Encoder\n",
    "    encoder_inputs = Input(shape=(None,), name=\"encoder_inputs\")\n",
    "    enc_emb = Embedding(num_encoder_tokens, config.input_embedding_size, name=\"encoder_embedding\")(encoder_inputs)\n",
    "\n",
    "    # Select RNN cell type\n",
    "    if config.cell_type == \"LSTM\":\n",
    "        RNNCellEncoder = LSTM\n",
    "        RNNCellDecoder = LSTM\n",
    "    elif config.cell_type == \"GRU\":\n",
    "        RNNCellEncoder = GRU\n",
    "        RNNCellDecoder = GRU\n",
    "    else:\n",
    "        RNNCellEncoder = keras.layers.SimpleRNN\n",
    "        RNNCellDecoder = keras.layers.SimpleRNN\n",
    "\n",
    "    # For simplicity and effective attention, encoder returns all sequences.\n",
    "    # Using a single layer for encoder as suggested for simplicity, but configurable.\n",
    "    encoder_outputs_list = []\n",
    "    current_encoder_output = enc_emb\n",
    "    encoder_states = [] # For the final state\n",
    "\n",
    "    for i in range(config.encoder_layers):\n",
    "        is_last_layer = (i == config.encoder_layers - 1)\n",
    "        encoder_rnn = RNNCellEncoder(config.hidden_size,\n",
    "                                     return_sequences=True, # Crucial for attention\n",
    "                                     return_state=True,\n",
    "                                     dropout=config.dropout_rate,\n",
    "                                     name=f\"encoder_{config.cell_type}_{i}\")\n",
    "        if config.cell_type == \"LSTM\":\n",
    "            current_encoder_output, state_h, state_c = encoder_rnn(current_encoder_output)\n",
    "            if is_last_layer:\n",
    "                encoder_states = [state_h, state_c]\n",
    "        else: # GRU or SimpleRNN\n",
    "            current_encoder_output, state_h = encoder_rnn(current_encoder_output)\n",
    "            if is_last_layer:\n",
    "                encoder_states = [state_h]\n",
    "    \n",
    "    encoder_all_outputs = current_encoder_output # These are the 'values' for attention\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = Input(shape=(None,), name=\"decoder_inputs\")\n",
    "    dec_emb_layer = Embedding(num_decoder_tokens, config.input_embedding_size, name=\"decoder_embedding\")\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    # Decoder RNN - single layer as suggested for simplicity, but configurable\n",
    "    # It will process one timestep at a time in the training model for clarity,\n",
    "    # though Keras handles the loop.\n",
    "    decoder_rnn_layer = RNNCellDecoder(config.hidden_size,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   dropout=config.dropout_rate,\n",
    "                                   name=f\"decoder_{config.cell_type}\")\n",
    "\n",
    "    # Attention Layer\n",
    "    attention_layer = BahdanauAttention(config.hidden_size) # Attention units often same as hidden_size\n",
    "\n",
    "    all_decoder_outputs = []\n",
    "    \n",
    "    # Initial decoder hidden state from last encoder state\n",
    "    # If multiple encoder layers, encoder_states will be from the *last* encoder layer.\n",
    "    # If multiple decoder layers, this initial state is for the *first* decoder layer.\n",
    "    # For this example, we'll assume single layer decoder for state handling ease.\n",
    "    # If decoder_layers > 1, state management becomes more complex in custom loops.\n",
    "    # For now, let's make decoder_layers = 1 for this example to match the \"simplicity\" note.\n",
    "    # If config.decoder_layers > 1, the loop below needs careful state piping.\n",
    "    \n",
    "    # For training, Keras handles the looping over timesteps.\n",
    "    # We need to prepare the inputs for the attention mechanism.\n",
    "    # The decoder's hidden state at each step `t` will query the encoder_all_outputs.\n",
    "\n",
    "    # We need a setup that allows Keras to build the graph.\n",
    "    # Using an RNN with a custom cell or a more explicit loop with functional API.\n",
    "    # Keras `RNN` layer with a custom cell is one way.\n",
    "    # Another is to use the `AdditiveAttention` layer if we want Bahdanau, or `Attention` for Luong.\n",
    "    \n",
    "    # Simpler approach for Keras:\n",
    "    # Decoder RNN runs first, then its output is used with context.\n",
    "    # Let's use the Keras built-in AdditiveAttention or Attention layer for conciseness.\n",
    "    # `tf.keras.layers.Attention` is Luong-style (multiplicative/dot-product).\n",
    "    # `tf.keras.layers.AdditiveAttention` is Bahdanau-style.\n",
    "    \n",
    "    # For teaching purposes, let's try a manual loop-like structure within functional API for one step,\n",
    "    # then adapt to how Keras expects full sequences.\n",
    "\n",
    "    # Correct way for training with Keras layers:\n",
    "    # The decoder RNN will take the embedded input and its previous state.\n",
    "    # Its output (query) and encoder_all_outputs (values) go to attention.\n",
    "    # The context vector from attention is concatenated with RNN output and fed to Dense.\n",
    "\n",
    "    # Initial decoder hidden state:\n",
    "    decoder_hidden_states = encoder_states # From last encoder layer\n",
    "\n",
    "    # Decoder processing loop (conceptual for training; Keras handles this with return_sequences=True)\n",
    "    # For TF functional API, we'd pass the whole sequence\n",
    "    \n",
    "    # Let's use the decoder_rnn_layer to process the entire sequence of decoder embeddings\n",
    "    if config.cell_type == \"LSTM\":\n",
    "        decoder_rnn_outputs, _, _ = decoder_rnn_layer(dec_emb, initial_state=decoder_hidden_states)\n",
    "    else: # GRU / SimpleRNN\n",
    "        decoder_rnn_outputs, _ = decoder_rnn_layer(dec_emb, initial_state=decoder_hidden_states)\n",
    "    \n",
    "    # decoder_rnn_outputs shape: (batch_size, target_seq_len, decoder_hidden_size)\n",
    "    # encoder_all_outputs shape: (batch_size, input_seq_len, encoder_hidden_size)\n",
    "    # Note: For Bahdanau, encoder_hidden_size and decoder_hidden_size can be different.\n",
    "    # Our BahdanauAttention layer takes W1(values) and W2(query).\n",
    "    # If hidden_size is same for encoder and decoder, it simplifies. Assume config.hidden_size is used for both.\n",
    "    \n",
    "    # Using tf.keras.layers.AdditiveAttention (Bahdanau-style)\n",
    "    # query is decoder_rnn_outputs, value is encoder_all_outputs\n",
    "    # The attention layer will compute context vector for each decoder timestep.\n",
    "    context_vector_seq, attention_weights_seq = tf.keras.layers.AdditiveAttention(name=\"attention_layer\")(\n",
    "        [decoder_rnn_outputs, encoder_all_outputs], return_attention_scores=True\n",
    "    )\n",
    "    # context_vector_seq shape: (batch_size, target_seq_len, encoder_hidden_size)\n",
    "    # attention_weights_seq shape: (batch_size, target_seq_len, input_seq_len)\n",
    "\n",
    "    # Concatenate context vector with decoder RNN output\n",
    "    decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")([decoder_rnn_outputs, context_vector_seq])\n",
    "\n",
    "    # Final output layer\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\", name=\"decoder_output_dense\")\n",
    "    decoder_outputs_final = decoder_dense(decoder_concat_input)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs_final)\n",
    "    \n",
    "    # Compile\n",
    "    optimizer_choice = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
    "    # Allow other optimizers from config if specified\n",
    "    if hasattr(config, 'optimizer'):\n",
    "        if config.optimizer == 'nadam':\n",
    "            optimizer_choice = tf.keras.optimizers.Nadam(learning_rate=config.learning_rate)\n",
    "        elif config.optimizer == 'rmsprop':\n",
    "            optimizer_choice = tf.keras.optimizers.RMSprop(learning_rate=config.learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=optimizer_choice, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Quick test of model building (optional, for dev)\n",
    "# class DummyConfig:\n",
    "#     input_embedding_size = 64\n",
    "#     hidden_size = 128\n",
    "#     encoder_layers = 1\n",
    "#     decoder_layers = 1 # For attention model, decoder RNN is often single layer before attention combination\n",
    "#     cell_type = \"GRU\"\n",
    "#     dropout_rate = 0.1\n",
    "#     learning_rate = 0.001\n",
    "#     optimizer = \"adam\"\n",
    "# test_config = DummyConfig()\n",
    "# if num_encoder_tokens > 0 and num_decoder_tokens > 0 : # Only if vocab is built\n",
    "#     try:\n",
    "#         test_attention_model = build_attention_seq2seq_model(test_config)\n",
    "#         test_attention_model.summary()\n",
    "#         print(\"Attention model built successfully.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error building attention model: {e}\")\n",
    "# else:\n",
    "#     print(\"Vocab not built, skipping model build test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8d8f421-2c1e-4f94-a89a-bd753ce2db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Inference Models for Attention Model\n",
    "\n",
    "def build_attention_inference_models(training_model, config):\n",
    "    # --- Encoder Model ---\n",
    "    encoder_inputs_inf = training_model.get_layer(\"encoder_inputs\").input\n",
    "    encoder_embedding_inf = training_model.get_layer(\"encoder_embedding\")(encoder_inputs_inf)\n",
    "    \n",
    "    current_encoder_output_inf = encoder_embedding_inf\n",
    "    encoder_states_inf = [] # Final states of the last encoder layer\n",
    "    \n",
    "    for i in range(config.encoder_layers):\n",
    "        encoder_rnn_layer_inf = training_model.get_layer(f\"encoder_{config.cell_type}_{i}\")\n",
    "        if config.cell_type == \"LSTM\":\n",
    "            current_encoder_output_inf, state_h_enc, state_c_enc = encoder_rnn_layer_inf(current_encoder_output_inf)\n",
    "            if i == config.encoder_layers - 1: # Last layer\n",
    "                encoder_states_inf = [state_h_enc, state_c_enc]\n",
    "        else: # GRU or SimpleRNN\n",
    "            current_encoder_output_inf, state_h_enc = encoder_rnn_layer_inf(current_encoder_output_inf)\n",
    "            if i == config.encoder_layers - 1: # Last layer\n",
    "                encoder_states_inf = [state_h_enc]\n",
    "    \n",
    "    encoder_all_outputs_inf = current_encoder_output_inf # All hidden states from last encoder layer\n",
    "    encoder_model_inf = Model(encoder_inputs_inf, [encoder_all_outputs_inf] + encoder_states_inf)\n",
    "\n",
    "    # --- Decoder Model for Inference (step-by-step) ---\n",
    "    decoder_hidden_size = config.hidden_size # Assuming decoder hidden size is same as attention units\n",
    "    \n",
    "    # Inputs for the decoder step\n",
    "    decoder_input_single_step = Input(shape=(1,), name=\"decoder_input_single_step\") # One token\n",
    "    encoder_all_outputs_as_input = Input(shape=(max_encoder_seq_length, config.hidden_size), name=\"encoder_all_outputs_as_input\") # From encoder\n",
    "\n",
    "    # Decoder initial states (list of tensors, one for h, one for c if LSTM)\n",
    "    decoder_initial_states_inputs = []\n",
    "    if config.cell_type == \"LSTM\":\n",
    "        decoder_state_input_h = Input(shape=(decoder_hidden_size,), name=\"decoder_state_h_input\")\n",
    "        decoder_state_input_c = Input(shape=(decoder_hidden_size,), name=\"decoder_state_c_input\")\n",
    "        decoder_initial_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    else: # GRU or SimpleRNN\n",
    "        decoder_state_input_h = Input(shape=(decoder_hidden_size,), name=\"decoder_state_h_input\")\n",
    "        decoder_initial_states_inputs = [decoder_state_input_h]\n",
    "\n",
    "    # Get layers from the trained model\n",
    "    dec_emb_layer_inf = training_model.get_layer(\"decoder_embedding\")\n",
    "    decoder_rnn_layer_inf = training_model.get_layer(f\"decoder_{config.cell_type}\") # Assumes single layer decoder for simplicity\n",
    "    attention_layer_inf = training_model.get_layer(\"attention_layer\") # Keras AdditiveAttention\n",
    "    decoder_dense_inf = training_model.get_layer(\"decoder_output_dense\")\n",
    "    concat_layer_inf = training_model.get_layer(\"concat_layer\")\n",
    "\n",
    "\n",
    "    # Decoder step execution\n",
    "    dec_emb_single_step = dec_emb_layer_inf(decoder_input_single_step) # Shape (batch, 1, embedding_dim)\n",
    "\n",
    "    # Decoder RNN step\n",
    "    if config.cell_type == \"LSTM\":\n",
    "        decoder_rnn_output_step, state_h_dec, state_c_dec = decoder_rnn_layer_inf(\n",
    "            dec_emb_single_step, initial_state=decoder_initial_states_inputs\n",
    "        )\n",
    "        decoder_new_states = [state_h_dec, state_c_dec]\n",
    "    else: # GRU or SimpleRNN\n",
    "        decoder_rnn_output_step, state_h_dec = decoder_rnn_layer_inf(\n",
    "            dec_emb_single_step, initial_state=decoder_initial_states_inputs\n",
    "        )\n",
    "        decoder_new_states = [state_h_dec]\n",
    "    # decoder_rnn_output_step shape (batch, 1, decoder_hidden_size)\n",
    "    \n",
    "    # Attention step\n",
    "    # query is decoder_rnn_output_step, value is encoder_all_outputs_as_input\n",
    "    context_vector_step, attention_weights_step = attention_layer_inf(\n",
    "        [decoder_rnn_output_step, encoder_all_outputs_as_input], return_attention_scores=True\n",
    "    )\n",
    "    # context_vector_step shape: (batch, 1, encoder_hidden_size)\n",
    "    # attention_weights_step shape: (batch, 1, input_seq_len) (for AdditiveAttention)\n",
    "    # Squeeze out the time dimension from attention weights for plotting: (batch, input_seq_len)\n",
    "    squeezed_attention_weights = tf.squeeze(attention_weights_step, axis=1)\n",
    "\n",
    "\n",
    "    # Concatenate context vector with decoder RNN output for this step\n",
    "    decoder_concat_input_step = concat_layer_inf([decoder_rnn_output_step, context_vector_step])\n",
    "    # decoder_concat_input_step shape (batch, 1, combined_hidden_size)\n",
    "\n",
    "    # Final dense layer for this step\n",
    "    decoder_output_final_step = decoder_dense_inf(decoder_concat_input_step) # Shape (batch, 1, num_decoder_tokens)\n",
    "    # Squeeze out the time dimension for output probabilities\n",
    "    squeezed_decoder_output = tf.squeeze(decoder_output_final_step, axis=1)\n",
    "\n",
    "\n",
    "    decoder_model_inf = Model(\n",
    "        [decoder_input_single_step, encoder_all_outputs_as_input] + decoder_initial_states_inputs, \n",
    "        [squeezed_decoder_output, squeezed_attention_weights] + decoder_new_states\n",
    "    )\n",
    "    \n",
    "    return encoder_model_inf, decoder_model_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30101215-a6b1-43c6-b8dc-df779ff70c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Beam Search Decode Function for Attention Model\n",
    "\n",
    "def decode_sequence_beam_search_attention(input_seq_vectorized, encoder_model, decoder_model, beam_width, config):\n",
    "    # Encode the input to get all encoder outputs and final states\n",
    "    encoder_outputs = encoder_model.predict(input_seq_vectorized, verbose=0)\n",
    "    encoder_all_hidden_states = encoder_outputs[0] # This is the `values` for attention\n",
    "    \n",
    "    # Initial decoder states come from the rest of encoder_outputs\n",
    "    # If LSTM: [state_h, state_c], if GRU/RNN: [state_h]\n",
    "    initial_decoder_states = encoder_outputs[1:] \n",
    "\n",
    "    # Start with the SOS token\n",
    "    start_token_idx = target_token_index[SOS_TOKEN]\n",
    "    \n",
    "    # Initial beam: (sequence_indices, log_probability, last_decoder_states, list_of_attention_weights_for_seq)\n",
    "    # Sequence is a list of token indices\n",
    "    initial_beam = [([start_token_idx], 0.0, initial_decoder_states, [])] \n",
    "    live_hypotheses = initial_beam\n",
    "\n",
    "    completed_hypotheses = []\n",
    "\n",
    "    for _ in range(max_decoder_seq_length): # Max decode steps\n",
    "        new_hypotheses_candidates = []\n",
    "        \n",
    "        # If all live hypotheses have ended or no live hypotheses left\n",
    "        if not live_hypotheses or all(h[0][-1] == target_token_index[EOS_TOKEN] for h in live_hypotheses if len(h[0]) > 1):\n",
    "            break\n",
    "\n",
    "        for seq_indices, score, current_decoder_states, attn_weights_list in live_hypotheses:\n",
    "            # If EOS token is the last token, this hypothesis is complete\n",
    "            if seq_indices[-1] == target_token_index[EOS_TOKEN] and len(seq_indices) > 1:\n",
    "                completed_hypotheses.append((seq_indices, score / (len(seq_indices)-1), current_decoder_states, attn_weights_list)) # Normalize score\n",
    "                continue # Don't expand completed hypotheses\n",
    "\n",
    "            # Prepare decoder input for the next step\n",
    "            last_token_idx_input = np.array([[seq_indices[-1]]])\n",
    "            \n",
    "            decoder_model_inputs = [last_token_idx_input, encoder_all_hidden_states] + current_decoder_states\n",
    "            \n",
    "            # Predict next token probabilities, attention_weights, and new decoder states\n",
    "            decoder_pred_outputs = decoder_model.predict(decoder_model_inputs, verbose=0)\n",
    "            \n",
    "            output_token_probs = decoder_pred_outputs[0]      # Shape (batch=1, num_decoder_tokens)\n",
    "            attention_weights_step = decoder_pred_outputs[1]  # Shape (batch=1, input_seq_len)\n",
    "            new_decoder_states = decoder_pred_outputs[2:]     # List of state tensors\n",
    "\n",
    "            # Using log probabilities\n",
    "            log_probs = np.log(output_token_probs[0] + 1e-9) # Add epsilon for stability\n",
    "            \n",
    "            # Get top N candidates (N=beam_width)\n",
    "            top_k_indices = np.argsort(log_probs)[-beam_width:]\n",
    "            \n",
    "            for token_idx in top_k_indices:\n",
    "                new_seq_indices = seq_indices + [token_idx]\n",
    "                new_score = score + log_probs[token_idx]\n",
    "                new_attn_weights_list = attn_weights_list + [attention_weights_step[0]] # Store current step's attention\n",
    "                new_hypotheses_candidates.append((new_seq_indices, new_score, new_decoder_states, new_attn_weights_list))\n",
    "        \n",
    "        # Sort all candidates by score and prune to keep only beam_width\n",
    "        if new_hypotheses_candidates:\n",
    "            live_hypotheses = sorted(new_hypotheses_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        else: # No new candidates, all might have completed or something went wrong\n",
    "            live_hypotheses = [] \n",
    "\n",
    "    # Add any remaining live hypotheses to completed (if they didn't end with EOS but max_len reached)\n",
    "    for seq_indices, score, states, attn_list in live_hypotheses:\n",
    "         completed_hypotheses.append((seq_indices, score / (len(seq_indices)-1) if len(seq_indices) > 1 else score, states, attn_list))\n",
    "\n",
    "    if not completed_hypotheses: # Handle cases where no hypothesis completes (e.g. very short max_decoder_seq_length)\n",
    "        # Fallback: use the best live hypothesis if any, or return empty\n",
    "        if live_hypotheses:\n",
    "             best_hypothesis = max(live_hypotheses, key=lambda x: x[1]/len(x[0]) if len(x[0]) > 1 else x[1])\n",
    "        else:\n",
    "            return \"\", np.array([]) # Return empty string and empty attention weights\n",
    "    else:\n",
    "        # Choose the best hypothesis from completed ones (highest normalized score)\n",
    "        best_hypothesis = max(completed_hypotheses, key=lambda x: x[1])\n",
    "        \n",
    "    decoded_sentence_indices = best_hypothesis[0]\n",
    "    final_attention_weights_list = best_hypothesis[3] # List of attention arrays for each decoded step\n",
    "    \n",
    "    # Convert indices to characters\n",
    "    decoded_sentence = \"\"\n",
    "    for token_idx in decoded_sentence_indices:\n",
    "        if token_idx == target_token_index[SOS_TOKEN]:\n",
    "            continue\n",
    "        if token_idx == target_token_index[EOS_TOKEN]:\n",
    "            break\n",
    "        if token_idx in reverse_target_char_index:\n",
    "             decoded_sentence += reverse_target_char_index[token_idx]\n",
    "    \n",
    "    # Stack attention weights into a matrix: (target_len, input_len)\n",
    "    # Exclude SOS token's attention if it was part of the loop\n",
    "    # The length of final_attention_weights_list should match length of decoded_sentence (approx)\n",
    "    if final_attention_weights_list:\n",
    "        attention_matrix = np.array(final_attention_weights_list)\n",
    "        # Ensure attention_matrix is 2D, e.g. (len_output_seq, len_input_seq)\n",
    "        if attention_matrix.ndim == 1: # If only one step, may need reshape\n",
    "            attention_matrix = np.expand_dims(attention_matrix, axis=0)\n",
    "    else:\n",
    "        attention_matrix = np.array([]) # Empty if no decoding steps produced attention\n",
    "\n",
    "    return decoded_sentence, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b645291b-125c-48f3-a2d7-14adca6cfb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Training and Evaluation Function for Sweep (train_evaluate_attention)\n",
    "\n",
    "def train_evaluate_attention():\n",
    "    keras.backend.clear_session()\n",
    "    run = wandb.init() # Project/entity inherited from sweep\n",
    "    config = wandb.config\n",
    "\n",
    "    print(f\"--- Attention Model: Building model for run {run.id if run else 'N/A'} with config: {dict(config)} ---\")\n",
    "    attention_training_model = build_attention_seq2seq_model(config)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=config.early_stopping_patience, restore_best_weights=True, verbose=1)\n",
    "    wandb_metrics_logger = WandbMetricsLogger(log_freq=\"epoch\")\n",
    "\n",
    "    print(f\"--- Attention Model: Starting training for run {run.id if run else 'N/A'} ---\")\n",
    "    history = attention_training_model.fit(\n",
    "        [encoder_input_train, decoder_input_train],\n",
    "        decoder_target_train,\n",
    "        batch_size=config.batch_size,\n",
    "        epochs=config.epochs,\n",
    "        validation_data=([encoder_input_val, decoder_input_val], decoder_target_val),\n",
    "        callbacks=[early_stopping, wandb_metrics_logger],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    wandb.log({\"val_exact_match_accuracy_attention\": history.history['val_accuracy'][-1]})\n",
    "    \n",
    "#    # --- Evaluation with Beam Search (on validation set for sweep metric) ---\n",
    "#    print(f\"--- Attention Model: Building inference models for run {run.id if run else 'N/A'} ---\")\n",
    "#    encoder_model_inf, decoder_model_inf = build_attention_inference_models(attention_training_model, config)\n",
    "#\n",
    "#    correct_predictions = 0\n",
    "#    total_predictions = encoder_input_val.shape[0]\n",
    "#    \n",
    "#    if total_predictions == 0:\n",
    "#        print(\"No validation data for attention model evaluation.\")\n",
    "#        wandb.log({\"val_exact_match_accuracy_attention\": 0.0})\n",
    "#        return\n",
    "#\n",
    "#    eval_table_data_attention = []\n",
    "#    print(f\"--- Attention Model: Starting validation evaluation for run {run.id if run else 'N/A'} ---\")\n",
    "#    for i in range(min(total_predictions, 1000)): # Evaluate on a subset of val for speed during sweep\n",
    "#        current_input_vector = encoder_input_val[i:i+1]\n",
    "#        original_input_text = input_texts_val[i]\n",
    "#        original_target_text = target_texts_val[i]\n",
    "#        \n",
    "#        decoded_sentence, _ = decode_sequence_beam_search_attention( # We don't need attention weights here\n",
    "#            current_input_vector, encoder_model_inf, decoder_model_inf, config.beam_size, config\n",
    "#        )\n",
    "#        \n",
    "#        if decoded_sentence == original_target_text:\n",
    "#            correct_predictions += 1\n",
    "#        \n",
    "#        if i < 3 and run: # Log first 3 examples to W&B Table per run\n",
    "#            eval_table_data_attention.append([original_input_text, original_target_text, decoded_sentence])\n",
    "#\n",
    "#    if eval_table_data_attention and run:\n",
    "#        try:\n",
    "#            wandb.log({\"eval_examples_attention\": wandb.Table(data=eval_table_data_attention,\n",
    "#                                                       columns=[\"Input\", \"True Target\", \"Predicted Target\"])})\n",
    "#        except Exception as e:\n",
    "#            print(f\"Error logging eval_examples_attention to W&B: {e}\")  \n",
    "#\n",
    "#    val_exact_match_accuracy = correct_predictions / min(total_predictions, 1000) if min(total_predictions, 1000) > 0 else 0.0\n",
    "#    \n",
    "#    if run:\n",
    "#        try:\n",
    "#            wandb.log({\"val_exact_match_accuracy_attention\": val_exact_match_accuracy})\n",
    "#        except Exception as e:\n",
    "#            print(f\"Error logging val_exact_match_accuracy_attention to W&B: {e}\")\n",
    "#    \n",
    "#    print(f\"Run {run.id if run else 'Unknown'} | Attention Model Validation Accuracy (Beam {config.beam_size}): {val_exact_match_accuracy:.4f}\")\n",
    "#    print(f\"--- Attention Model: Finished evaluation for run {run.id if run else 'N/A'} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f627cf-1dd8-49fb-a333-7ef58570d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Wandb Sweep Configuration for Attention Model\n",
    "\n",
    "sweep_config_attention = {\n",
    "    'method': 'bayes',  # Bayesian optimization, or 'random', 'grid'\n",
    "    'metric': {\n",
    "        'name': 'val_exact_match_accuracy_attention', # Custom metric from beam search eval\n",
    "        'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'input_embedding_size': {\n",
    "            'values': [32, 64, 128] \n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [64, 128, 256] \n",
    "        },\n",
    "        'encoder_layers': {\n",
    "            'values': [1, 2]\n",
    "        },\n",
    "        'decoder_layers': {\n",
    "            'values': [1, 2]\n",
    "        },\n",
    "        'cell_type': {\n",
    "            'values': ['GRU', 'LSTM']\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'values': [0.2, 0.3]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.001]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [64, 128, 256]\n",
    "        },\n",
    "        'epochs': { # Max epochs, early stopping will handle actual duration\n",
    "            'values': [50] # Reduced for quicker sweep, increase for final model\n",
    "        },\n",
    "        'early_stopping_patience': {\n",
    "            'values': [5]\n",
    "        },\n",
    "        'beam_size': { # This is for evaluation\n",
    "            'values': [1, 3, 5] # 1 is greedy\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'nadam']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Note on hyperparameters for Q5(a):\n",
    "# Yes, hyperparameters should be tuned again for the attention model.\n",
    "# The optimal configuration for a vanilla seq2seq model might not be optimal\n",
    "# for a model with an attention mechanism due to differences in architecture,\n",
    "# parameter count, and learning dynamics. The sweep above is an example.\n",
    "# For simplicity, the problem statement suggests single-layer encoder/decoder.\n",
    "# The sweep config can be adjusted accordingly (e.g., 'encoder_layers': {'values': [1]}).\n",
    "# The current `build_attention_seq2seq_model` is designed for a single-layer decoder RNN\n",
    "# before combining with attention context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2416a-0e26-4e38-ae36-92a40f6be2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 1wuogarq\n",
      "Sweep URL: https://wandb.ai/ce21b097-indian-institute-of-technology-madras/CE21B097%20-%20DA6401%20-%20Assignment%203/sweeps/1wuogarq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: duke9oan with config:\n",
      "wandb: \tbatch_size: 64\n",
      "wandb: \tbeam_size: 3\n",
      "wandb: \tcell_type: GRU\n",
      "wandb: \tdecoder_layers: 1\n",
      "wandb: \tdropout_rate: 0.2\n",
      "wandb: \tearly_stopping_patience: 5\n",
      "wandb: \tencoder_layers: 1\n",
      "wandb: \tepochs: 50\n",
      "wandb: \thidden_size: 64\n",
      "wandb: \tinput_embedding_size: 128\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \toptimizer: nadam\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\prana\\Downloads\\DA6401 Assignment 3\\wandb\\run-20250520_211644-duke9oan</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ce21b097-indian-institute-of-technology-madras/CE21B097%20-%20DA6401%20-%20Assignment%203/runs/duke9oan' target=\"_blank\">visionary-sweep-1</a></strong> to <a href='https://wandb.ai/ce21b097-indian-institute-of-technology-madras/CE21B097%20-%20DA6401%20-%20Assignment%203' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ce21b097-indian-institute-of-technology-madras/CE21B097%20-%20DA6401%20-%20Assignment%203/sweeps/1wuogarq' target=\"_blank\">https://wandb.ai/ce21b097-indian-institute-of-technology-madras/CE21B097%20-%20DA6401%20-%20Assignment%203/sweeps/1wuogarq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ce21b097-indian-institute-of-technology-madras/CE21B097%20-%20DA6401%20-%20Assignment%203' target=\"_blank\">https://wandb.ai/ce21b097-indian-institute-of-technology-madras/CE21B097%20-%20DA6401%20-%20Assignment%203</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ce21b097-indian-institute-of-technology-madras/CE21B097%20-%20DA6401%20-%20Assignment%203/sweeps/1wuogarq' target=\"_blank\">https://wandb.ai/ce21b097-indian-institute-of-technology-madras/CE21B097%20-%20DA6401%20-%20Assignment%203/sweeps/1wuogarq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ce21b097-indian-institute-of-technology-madras/CE21B097%20-%20DA6401%20-%20Assignment%203/runs/duke9oan' target=\"_blank\">https://wandb.ai/ce21b097-indian-institute-of-technology-madras/CE21B097%20-%20DA6401%20-%20Assignment%203/runs/duke9oan</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Attention Model: Building model for run duke9oan with config: {'batch_size': 64, 'beam_size': 3, 'cell_type': 'GRU', 'decoder_layers': 1, 'dropout_rate': 0.2, 'early_stopping_patience': 5, 'encoder_layers': 1, 'epochs': 50, 'hidden_size': 64, 'input_embedding_size': 128, 'learning_rate': 0.001, 'optimizer': 'nadam'} ---\n",
      "--- Attention Model: Starting training for run duke9oan ---\n",
      "Epoch 1/50\n",
      "691/691 [==============================] - 17s 17ms/step - loss: 1.1076 - accuracy: 0.0742 - val_loss: 0.9680 - val_accuracy: 0.0806\n",
      "Epoch 2/50\n",
      "691/691 [==============================] - 11s 15ms/step - loss: 0.9812 - accuracy: 0.0903 - val_loss: 0.9317 - val_accuracy: 0.0874\n",
      "Epoch 3/50\n",
      "691/691 [==============================] - 10s 15ms/step - loss: 0.9563 - accuracy: 0.0939 - val_loss: 0.9185 - val_accuracy: 0.0895\n",
      "Epoch 4/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.9425 - accuracy: 0.0958 - val_loss: 0.9098 - val_accuracy: 0.0903\n",
      "Epoch 5/50\n",
      "691/691 [==============================] - 11s 15ms/step - loss: 0.9329 - accuracy: 0.0976 - val_loss: 0.9026 - val_accuracy: 0.0906\n",
      "Epoch 6/50\n",
      "691/691 [==============================] - 11s 15ms/step - loss: 0.9255 - accuracy: 0.0988 - val_loss: 0.8982 - val_accuracy: 0.0914\n",
      "Epoch 7/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.9195 - accuracy: 0.0999 - val_loss: 0.8937 - val_accuracy: 0.0925\n",
      "Epoch 8/50\n",
      "691/691 [==============================] - 12s 18ms/step - loss: 0.9143 - accuracy: 0.1009 - val_loss: 0.8905 - val_accuracy: 0.0939\n",
      "Epoch 9/50\n",
      "691/691 [==============================] - 12s 17ms/step - loss: 0.9097 - accuracy: 0.1021 - val_loss: 0.8874 - val_accuracy: 0.0943\n",
      "Epoch 10/50\n",
      "691/691 [==============================] - 11s 17ms/step - loss: 0.9058 - accuracy: 0.1029 - val_loss: 0.8858 - val_accuracy: 0.0948\n",
      "Epoch 11/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.9023 - accuracy: 0.1039 - val_loss: 0.8822 - val_accuracy: 0.0950\n",
      "Epoch 12/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.8990 - accuracy: 0.1047 - val_loss: 0.8811 - val_accuracy: 0.0952\n",
      "Epoch 13/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.8963 - accuracy: 0.1054 - val_loss: 0.8787 - val_accuracy: 0.0951\n",
      "Epoch 14/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.8936 - accuracy: 0.1060 - val_loss: 0.8778 - val_accuracy: 0.0958\n",
      "Epoch 15/50\n",
      "691/691 [==============================] - 12s 17ms/step - loss: 0.8913 - accuracy: 0.1064 - val_loss: 0.8759 - val_accuracy: 0.0956\n",
      "Epoch 16/50\n",
      "691/691 [==============================] - 11s 15ms/step - loss: 0.8890 - accuracy: 0.1071 - val_loss: 0.8748 - val_accuracy: 0.0953\n",
      "Epoch 17/50\n",
      "691/691 [==============================] - 11s 15ms/step - loss: 0.8869 - accuracy: 0.1073 - val_loss: 0.8734 - val_accuracy: 0.0948\n",
      "Epoch 18/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.8848 - accuracy: 0.1080 - val_loss: 0.8723 - val_accuracy: 0.0964\n",
      "Epoch 19/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.8831 - accuracy: 0.1082 - val_loss: 0.8715 - val_accuracy: 0.0966\n",
      "Epoch 20/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.8814 - accuracy: 0.1087 - val_loss: 0.8709 - val_accuracy: 0.0966\n",
      "Epoch 21/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.8799 - accuracy: 0.1090 - val_loss: 0.8706 - val_accuracy: 0.0957\n",
      "Epoch 22/50\n",
      "691/691 [==============================] - 11s 15ms/step - loss: 0.8784 - accuracy: 0.1094 - val_loss: 0.8698 - val_accuracy: 0.0962\n",
      "Epoch 23/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.8771 - accuracy: 0.1097 - val_loss: 0.8688 - val_accuracy: 0.0965\n",
      "Epoch 24/50\n",
      "691/691 [==============================] - 10s 15ms/step - loss: 0.8758 - accuracy: 0.1097 - val_loss: 0.8702 - val_accuracy: 0.0960\n",
      "Epoch 25/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.8682 - accuracy: 0.1119 - val_loss: 0.8238 - val_accuracy: 0.1108\n",
      "Epoch 26/50\n",
      "691/691 [==============================] - 12s 17ms/step - loss: 0.7950 - accuracy: 0.1345 - val_loss: 0.7755 - val_accuracy: 0.1242\n",
      "Epoch 27/50\n",
      "691/691 [==============================] - 12s 17ms/step - loss: 0.7771 - accuracy: 0.1372 - val_loss: 0.7663 - val_accuracy: 0.1252\n",
      "Epoch 28/50\n",
      "691/691 [==============================] - 12s 17ms/step - loss: 0.7696 - accuracy: 0.1390 - val_loss: 0.7630 - val_accuracy: 0.1252\n",
      "Epoch 29/50\n",
      "691/691 [==============================] - 11s 17ms/step - loss: 0.7666 - accuracy: 0.1392 - val_loss: 0.7611 - val_accuracy: 0.1249\n",
      "Epoch 30/50\n",
      "691/691 [==============================] - 12s 18ms/step - loss: 0.7645 - accuracy: 0.1394 - val_loss: 0.7597 - val_accuracy: 0.1253\n",
      "Epoch 31/50\n",
      "691/691 [==============================] - 12s 18ms/step - loss: 0.7629 - accuracy: 0.1397 - val_loss: 0.7595 - val_accuracy: 0.1255\n",
      "Epoch 32/50\n",
      "691/691 [==============================] - 12s 17ms/step - loss: 0.7618 - accuracy: 0.1398 - val_loss: 0.7579 - val_accuracy: 0.1254\n",
      "Epoch 33/50\n",
      "691/691 [==============================] - 11s 17ms/step - loss: 0.7601 - accuracy: 0.1401 - val_loss: 0.7581 - val_accuracy: 0.1250\n",
      "Epoch 34/50\n",
      "691/691 [==============================] - 12s 17ms/step - loss: 0.7588 - accuracy: 0.1403 - val_loss: 0.7575 - val_accuracy: 0.1248\n",
      "Epoch 35/50\n",
      "691/691 [==============================] - 12s 17ms/step - loss: 0.7578 - accuracy: 0.1406 - val_loss: 0.7567 - val_accuracy: 0.1252\n",
      "Epoch 36/50\n",
      "691/691 [==============================] - 12s 17ms/step - loss: 0.7565 - accuracy: 0.1406 - val_loss: 0.7548 - val_accuracy: 0.1248\n",
      "Epoch 37/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.7556 - accuracy: 0.1406 - val_loss: 0.7547 - val_accuracy: 0.1256\n",
      "Epoch 38/50\n",
      "691/691 [==============================] - 11s 17ms/step - loss: 0.7544 - accuracy: 0.1409 - val_loss: 0.7548 - val_accuracy: 0.1246\n",
      "Epoch 39/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.7536 - accuracy: 0.1410 - val_loss: 0.7534 - val_accuracy: 0.1255\n",
      "Epoch 40/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.7526 - accuracy: 0.1414 - val_loss: 0.7550 - val_accuracy: 0.1254\n",
      "Epoch 41/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.7521 - accuracy: 0.1414 - val_loss: 0.7533 - val_accuracy: 0.1252\n",
      "Epoch 42/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.7511 - accuracy: 0.1415 - val_loss: 0.7525 - val_accuracy: 0.1250\n",
      "Epoch 43/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.7503 - accuracy: 0.1416 - val_loss: 0.7527 - val_accuracy: 0.1249\n",
      "Epoch 44/50\n",
      "691/691 [==============================] - 12s 17ms/step - loss: 0.7496 - accuracy: 0.1416 - val_loss: 0.7519 - val_accuracy: 0.1255\n",
      "Epoch 45/50\n",
      "691/691 [==============================] - 11s 16ms/step - loss: 0.7489 - accuracy: 0.1419 - val_loss: 0.7518 - val_accuracy: 0.1256\n",
      "Epoch 46/50\n",
      " 20/691 [..............................] - ETA: 11s - loss: 0.7485 - accuracy: 0.1398"
     ]
    }
   ],
   "source": [
    "# Cell 11: Start Sweep Agent for Attention Model\n",
    "\n",
    "# --- Initialize Sweep for Attention Model ---\n",
    "# Ensure your W&B entity and project are correctly set.\n",
    "# You might want a new project name or a way to distinguish these runs.\n",
    "sweep_id_attention = wandb.sweep(\n",
    "    sweep_config_attention, \n",
    "    entity=\"ce21b097-indian-institute-of-technology-madras\", # Replace with your entity\n",
    "    project=\"CE21B097 - DA6401 - Assignment 3\" # Example: New project or tag runs\n",
    ")\n",
    "\n",
    "# --- Run Agent for Attention Model ---\n",
    "# Adjust 'count' as needed.\n",
    "wandb.agent(sweep_id_attention, function=train_evaluate_attention, count=15) # Example: 10 runs\n",
    "\n",
    "print(\"\\n--- Attention Model Sweep Finished ---\")\n",
    "print(\"Go to your W&B project page to see the results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d4ae17-e435-4094-a6b8-7c3f255f5a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
